{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAmiWJ5yRL-3"
      },
      "source": [
        "**Scenario - 1 :**\n",
        "\n",
        "Hope, you are getting more and more comfortable working with Series and Dataframes. Now, since the data quality checks are taken care of, It's time we perform some real world data wrangling to generate some reports/insights etc.\n",
        "\n",
        "Check the code snippets below where we are reading customers, orders and transactions data. Click [here](https://raw.githubusercontent.com/mentorskool/python-essentials/main/Module-6-PY-Learning-4/Data/globalmart-business-data.xlsx) to download the Excel sheet containing the datasets\n",
        "\n",
        "You are required to answer the below questions :      \n",
        "* How many customers belong to Corporate segment?\n",
        "* How many customers from Corporate segment also belong to the outlook email domain?\n",
        "* Report segment wise count of customers. Which segment has highest number of customers?\n",
        "* Which segment of customers has placed the highest number of orders?\n",
        "* Prepare a summary to show month-year wise (based on order purchase date) orders delivered by different ship modes for the year 2017\n",
        "\n",
        "Expected Outcome :  \n",
        "\n",
        "![](https://msklbusinessdata.blob.core.windows.net/python-masterclass/12-pivot-01.png)   \n",
        "\n",
        "* Enhance the same report to show the ship modes under various Customer segments\n",
        "\n",
        "Expected Outcome :\n",
        "\n",
        "![](https://msklbusinessdata.blob.core.windows.net/python-masterclass/12-pivot-02.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekFL9gSjbCKS"
      },
      "source": [
        "**Important Note :**\n",
        "\n",
        "We are going to perform familiar data operations like Filtering, Sorting, Merging (Excel like Vlookup or Joins in SQL), Group-Aggregation, Pivoting etc.\n",
        "\n",
        "You must have already done these tasks while working with data in Excel or SQL\n",
        "\n",
        "Let's see how Python performs these tasks using the famous Pandas library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gqLF42LzVfx6"
      },
      "outputs": [],
      "source": [
        "# Let's import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6rTI8GI3Vim9"
      },
      "outputs": [],
      "source": [
        "# reading customers data into a new dataframe customers\n",
        "\n",
        "customers = pd.read_excel('https://raw.githubusercontent.com/mentorskool/python-essentials/main/Module-6-PY-Learning-4/Data/globalmart-business-data.xlsx', sheet_name = 'customers')\n",
        "\n",
        "# reading orders data into a new dataframe orders\n",
        "\n",
        "orders = pd.read_excel('https://raw.githubusercontent.com/mentorskool/python-essentials/main/Module-6-PY-Learning-4/Data/globalmart-business-data.xlsx', sheet_name = 'orders')\n",
        "\n",
        "# reading transactions data into a new dataframe transactions\n",
        "\n",
        "transactions = pd.read_excel('https://raw.githubusercontent.com/mentorskool/python-essentials/main/Module-6-PY-Learning-4/Data/globalmart-business-data.xlsx', sheet_name = 'transactions')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DQRAhPrfnZjp"
      },
      "outputs": [],
      "source": [
        "# also we can load all sheets into one, and then one by one access each sheet from that\n",
        "# globalmart_data = pd.read_excel('https://raw.githubusercontent.com/mentorskool/python-essentials/main/Module-6-PY-Learning-4/Data/globalmart-business-data.xlsx', sheet_name=None)\n",
        "\n",
        "# customers = globalmart_data['customers']\n",
        "\n",
        "# orders = globalmart_data['orders']\n",
        "\n",
        "# transactions = globalmart_data['transactions']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kj8B3srZWcOE"
      },
      "outputs": [],
      "source": [
        "# Check the top 5 rows in customers dataframe\n",
        "\n",
        "customers.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Miv8YOKdWcLx"
      },
      "outputs": [],
      "source": [
        "# Check the top 5 rows in orders dataframe\n",
        "\n",
        "orders.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dINHY_HpWcJ1"
      },
      "outputs": [],
      "source": [
        "# Check the top 5 rows in transactions dataframe\n",
        "\n",
        "transactions.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yG6vwjgbrzym"
      },
      "source": [
        "### **Task 1** - How many customers belong to Corporate segment?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpIbt-_3r2YH"
      },
      "outputs": [],
      "source": [
        "# Filter column named segment for values = 'Corporate'\n",
        "# Output will be dataframe with only those records where segment = 'Corporate'\n",
        "\n",
        "customers[customers['segment'] == 'Corporate']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ViGg_VCvWcHn"
      },
      "outputs": [],
      "source": [
        "# Count all records which satisfy the condition stated above\n",
        "\n",
        "len(customers[customers['segment'] == 'Corporate'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZd_VNtDr5nx"
      },
      "source": [
        "### **Task 2** - How many customers from Corporate segment also belong to the outlook email domain?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g65Gl613WcFc"
      },
      "outputs": [],
      "source": [
        "# Split customer_email column on the character @ and fetch the part on the right of '@'\n",
        "# Store the fetched part into a new column called 'email_domain'\n",
        "\n",
        "customers['email_domain'] = customers['customer_email'].str.split('@',expand = True)[1].str.split('.',expand = True)[0]\n",
        "customers.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-I_KdLi-r-8h"
      },
      "outputs": [],
      "source": [
        "# Now fetch records from cutomers dataframe only those records where segment = 'Corporate'\n",
        "# And email_domain = 'outlook'\n",
        "\n",
        "customers[(customers['segment'] == 'Corporate') & (customers['email_domain'] == 'outlook')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_fnKrAArvLQ"
      },
      "outputs": [],
      "source": [
        "# Use len to fetch the count of records matching the condition stated above\n",
        "\n",
        "len(customers[(customers['segment'] == 'Corporate') & (customers['email_domain'] == 'outlook')])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pr8l21B5sVG6"
      },
      "source": [
        "### **Task 3** - Report segment wise count of customers. Which segment has highest number of customers?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLKE4MUpsTrg"
      },
      "outputs": [],
      "source": [
        "# Approach 1\n",
        "\n",
        "customers['segment'].value_counts()\n",
        "\n",
        "# Note how the output looks similar to group by and aggregate(count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QL6zymMgWcDS"
      },
      "outputs": [],
      "source": [
        "# Approach 2\n",
        "\n",
        "customers.groupby(['segment'])['customer_id'].count().reset_index()\n",
        "\n",
        "# Note how similar is the syntax to a typical group-by statement in SQL\n",
        "# Check the snippers : .groupby() and .count()\n",
        "# Why reset_index()?\n",
        "# Try once without reset_index() and check if the output is any different"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzlc07n8syhR"
      },
      "source": [
        "### **Task 4** - Which segment of customers has placed the highest number of orders?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C34pTfROtWDG"
      },
      "source": [
        "You have to merge two tables together: Customer and Order\n",
        "\n",
        "How do you achieve the same in SQL?\n",
        "* By making use of JOINS\n",
        "\n",
        "How do you achieve the same in Excel?\n",
        "* By making use of VLOOKUP() function\n",
        "\n",
        "\n",
        "Here customer_id of df1 (customer_detalis) is the common key , by which we can combine two dataframes\n",
        "\n",
        "Before go deep into code, let's explore this documentation : https://pandas.pydata.org/docs/user_guide/merging.html\n",
        "\n",
        "Did you figure out different ways to achieve merging of data?\n",
        "\n",
        "Below are three key methods :\n",
        "\n",
        "* Concatenate\n",
        "  * Just combining dataframes\n",
        "  * Index is not reset\n",
        "  * Default axis is 0\n",
        "* Merge\n",
        "  * pandas has full-featured, high performance in-memory join operations idiomatically very similar to relational databases like SQL.\n",
        "  * These methods perform significantly better (in some cases well over an order of magnitude better) than other open source implementations (like base::merge.data.frame in R).\n",
        "  * The reason for this is careful algorithmic design and the internal layout of the data in DataFrame.\n",
        "* Join\n",
        "  * This is a convenient method for combining the columns DataFrames into a single result DataFrame\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrlyOmhEWcBC"
      },
      "outputs": [],
      "source": [
        "# Approach 1\n",
        "# Put the dataframes customers and orders in a list and pass the list in the concat() method\n",
        "# concat() method looks for a column with matching names in both dataframes\n",
        "# Seems customer_id is the one having exactly matching name. Hence, it qualifies as the common key\n",
        "\n",
        "pd.concat([customers,orders])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsEwwO1Ptnst"
      },
      "source": [
        "Did you observe so many NaN values in the output dataframe?\n",
        "\n",
        "The primary reason of this being the nature of joins. Go back to your SQL skills and recall how left joins, right joins and full joins lead to lot of null values from the left, right or both tables\n",
        "\n",
        "What if we are only concerned about records with matching customer_ids. This would need an inner join to be performed.\n",
        "\n",
        "Let's check the code below to perform inner join in Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2qU21BTuFOY"
      },
      "outputs": [],
      "source": [
        "# Approach 2\n",
        "pd.concat([customers,orders], axis = 1, join = 'inner')\n",
        "\n",
        "# Figure out the reason why axis = 1 is important to add here. Experiment without axis = 1 and see if that impacts the code in anyway"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poAetWN1uPWB"
      },
      "source": [
        "But we are loosing some information of reviews data\n",
        "\n",
        "But by default concat have *two type of join* : **inner and outer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MH3dgxqrus8H"
      },
      "source": [
        "#### Type of Merges :    \n",
        "In the context of merging datasets, there are different types of relationships that can occur between the merge keys in the left and right datasets. These relationships are classified as follows:\n",
        "\n",
        "* **\"One-to-One\"** or **\"1:1\"**: This relationship type checks if the merge keys are unique in both the left and right datasets.\n",
        "\n",
        "* **\"One-to-Many\"** or **\"1:m\"**: This relationship type checks if the merge keys are unique in the left dataset.\n",
        "\n",
        "* **\"Many-to-One\"** or **\"m:1\"**: This relationship type checks if the merge keys are unique in the right dataset.\n",
        "\n",
        "* **\"Many-to-Many\"** or **\"m:m\"**: This relationship type is allowed, but it does not result in any specific checks.\n",
        "\n",
        "These relationship types help determine the uniqueness and compatibility of merge keys between datasets, ensuring the appropriate merging and data combination processes.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Approach 3\n",
        "orders.merge(customers, on='customer_id', validate='one_to_one')\n",
        "\n",
        "# Can you figure out why this code would fail?"
      ],
      "metadata": {
        "id": "62zk7jS2rhkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxcPwoHbuFSG"
      },
      "outputs": [],
      "source": [
        "# one_to_one not possible , because customer_id is not unique in orders dataset. One customer can place multiple orders and hence customer_id can repeat\n",
        "# customer: customer_id - unique\n",
        "# order: customer_id - many\n",
        "\n",
        "# Approach 4\n",
        "\n",
        "customers.merge(orders, on='customer_id', validate='1:m')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzrIGPz9uFXo"
      },
      "outputs": [],
      "source": [
        "# Approach 5\n",
        "\n",
        "orders.merge(customers, on='customer_id', validate='m:1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcuKcKXCvHIJ"
      },
      "source": [
        "* left   : LEFT OUTER JOIN    : Use whole keys from left frame and intersection of keys from right frame\n",
        "* right  : RIGHT OUTER JOIN   : Use whole keys from right frame and intersection of keys from left frame\n",
        "* outer  : FULL OUTER JOIN    : Use union of keys from both frames\n",
        "* inner  : INNER JOIN         : Use intersection of keys from both frames : default"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLbDqY3cnZj4"
      },
      "source": [
        "**Here are the commonly used join types and their behaviors:**\n",
        "* **Left Outer Join**: In this type of join, all the keys from the left dataset are included in the resulting dataset, along with the intersection of keys from the right dataset.\n",
        "\n",
        "* **Right Outer Join**: This join type is similar to the left outer join, but the keys from the right dataset are included in the resulting dataset, along with the intersection of keys from the left dataset.\n",
        "\n",
        "* **Full Outer Join**: With a full outer join, the resulting dataset includes the union of keys from both the left and right datasets. It combines all the keys and values from both datasets, including the intersection of keys.\n",
        "\n",
        "* **Inner Join**: The inner join is the default join type. It includes only the intersection of keys from both the left and right datasets. In other words, it includes only the keys and values that have a match in both datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TOtaB_IuFa5"
      },
      "outputs": [],
      "source": [
        "# Check if this code is a valid one?\n",
        "\n",
        "customers.merge(orders, on='customer_id', validate='one_to_many', how='outer')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djNDQ_weuFdh"
      },
      "outputs": [],
      "source": [
        "# How about this one?\n",
        "\n",
        "customers.merge(orders, on='customer_id', validate='one_to_many', how='inner')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgb5WjcRvQ6H"
      },
      "source": [
        "Strange! You never faced wrong output message like the one found with concat() method.\n",
        "\n",
        "The reason being, there we combined two dataframes without mentioning the primary key but in the case of merge(), we mentioned the column name on which merge to take place. Hence, we are getting the correct outcome."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oh8qJWK7uFhF"
      },
      "outputs": [],
      "source": [
        "# Approach 6\n",
        "\n",
        "pd.merge(orders,customers,on=['customer_id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "VxZtJtnTvawp"
      },
      "outputs": [],
      "source": [
        "# Final merged dataframe\n",
        "\n",
        "df = pd.merge(orders,customers,on=['customer_id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZykLy15Rvi0J"
      },
      "outputs": [],
      "source": [
        "# Filter the dataframe on segment column\n",
        "# And save the output in a new variable df1\n",
        "df1 = df[df['segment']=='Corporate']\n",
        "\n",
        "# Apply group-by on customer_id column and perform count on order_id column\n",
        "# Reset the index in the end and save the results back to df1 variable\n",
        "\n",
        "df1 = df1.groupby(['customer_id'])['order_id'].count().reset_index()\n",
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqeHeIwrwASA"
      },
      "outputs": [],
      "source": [
        "# Finally sort the dataframe df1 on order_id in descending order (Note ascending = False means descending)\n",
        "\n",
        "df1.sort_values('order_id', ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIocLbKxwN-f"
      },
      "source": [
        "### **Task 5** - Prepare a summary to show month-year wise (based on order purchase date) orders delivered by different ship modes for the year 2017"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kO0WcSGMSFAP"
      },
      "source": [
        "In order to attempt this task, we need to do some data preparation :      \n",
        "\n",
        "* We need to derive a column like Jan-2017 from a column with values like '15-01-2017'\n",
        "* We need to derive order_year and order_month in number format. Ex. for the value '15-01-2017', order_year will be 2017 and order_month will be 1\n",
        "* We need to ensure that we consider only those orders which are delivered\n",
        "* Finally, we need to sort the result from Jan-2017 to Dec-2017 ( in the order of month within an year)\n",
        "* Ensure that only those orders which were placed in 2017 are considered for this report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cf2BATNr00_o"
      },
      "outputs": [],
      "source": [
        "# Let's first check whether there are orders with status other than 'delivered'\n",
        "\n",
        "df['order_status'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "9K2BEblBbuRH"
      },
      "outputs": [],
      "source": [
        "# Prepare a new column showing date in MMM-YYYY format\n",
        "df['order_purchase_month_year'] = pd.DatetimeIndex(df['order_purchase_date']).strftime('%b-%Y')\n",
        "\n",
        "# Pull out order_purchase_month in number format\n",
        "df['order_purchase_month'] = pd.DatetimeIndex(df['order_purchase_date']).month\n",
        "\n",
        "# Pull out order_purchase_year in number format\n",
        "df['order_purchase_year'] = pd.DatetimeIndex(df['order_purchase_date']).year\n",
        "\n",
        "# Filter all orders which are delivered\n",
        "df['order_deliver']=np.where(df['order_status'] == 'delivered',1,0)\n",
        "\n",
        "# Filter all orders which are placed in the year 2017\n",
        "x=df[df['order_purchase_year'] == 2017]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzXdwP1RTXxi"
      },
      "source": [
        "Now, before we proceed, It's important to spend some time on learning how pivoting related tasks can be performed using Python.\n",
        "\n",
        "You must have come across situations when you have to pivot data to arrive at the desired result. The report format required as part of this task is also a typical outcome of pivoting activity.\n",
        "\n",
        "We'll highly recommend you to spend some time in [this](https://pbpython.com/pandas-pivot-table-explained.html) article which elaborates this topic quite well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VorSSLmU15Se"
      },
      "outputs": [],
      "source": [
        "# Based on your study of the article, try to decode what the below code snippets are doing?\n",
        "\n",
        "y=x.pivot_table(index=['order_purchase_month_year','order_purchase_month'],columns='ship_mode', values='order_deliver', aggfunc=np.sum)\n",
        "y=y.sort_values(by='order_purchase_month')\n",
        "y=y.reset_index()\n",
        "y.drop(['order_purchase_month'],axis=1,inplace=True)\n",
        "y.set_index('order_purchase_month_year')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6j8fakCEXv3"
      },
      "source": [
        "### **Task 6** - Enhance the same report to show the ship modes under various Customer segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xs3rf6s-cFCH"
      },
      "outputs": [],
      "source": [
        "# Check how this tasks is just an enhancement to the previous task. Most of the code remains the same\n",
        "\n",
        "y=x.pivot_table(index=['order_purchase_month_year','order_purchase_month'],columns=['segment','ship_mode'], values='order_deliver', aggfunc=np.sum)\n",
        "y=y.sort_values(by='order_purchase_month')\n",
        "y=y.reset_index()\n",
        "y.drop(['order_purchase_month'],axis=1,inplace=True)\n",
        "y.set_index('order_purchase_month_year')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}